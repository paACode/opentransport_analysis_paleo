---
title: "Paleo an opentransport Analysis"
author: "Leo Dost and Pascal Ackermann"
date: "30.01.2025"
output: html_document
---

```{r, echo=FALSE , include=FALSE}
## we remove the printing of the hashes in front of all results
library(knitr)
opts_chunk$set(echo = FALSE ,
               include = FALSE,
               comment = NA,
               eval = TRUE)
```

# Introduction

MadeUpCompany AG has recently identified that production employees frequently arrive late to work due to delays in public transportation. This poses a significant challenge since the absence of a single team member can halt the entire production line.

To address this issue, the Production Manager has initiated a research project in collaboration with HSLU.

Three possible countermeasure were identified:

-   Adjust shift start times based on transportation reliability.
-   Implement seasonal adjustments to production capacity based on weather-related transportation disruptions.
-   Consider train categories to mitigate dependency on unreliable train routes.
-   Use weather data and forecasts to predict possible delays

The goal of the Data Science students is to provide MadeUpCompany AG with an insightful report, analyzing publicly available data, to see which of the countermeasures could be most effective.

# Exploring Data

The first step in addressing the research question was to define the project scope and select suitable datasets. The analysis focuses specifically on public transport data for trains arriving in Lucerne, as MadeUpCompany AG is based there. Data from 2024 onward serves as the foundation for more advanced analysis. Additionally, weather data should reflect average Swiss values as a daily baseline.

For this study, data was sourced from [opentransportdata.swiss](https://opentransportdata.swiss/de/ist-daten-archiv/) and the [Schweizer Klimamessnetz](https://www.meteoschweiz.admin.ch/wetter/messsysteme/bodenstationen/schweizer-klimamessnetz.html).

## Tranport Data

[opentransportdata.swiss](https://opentransportdata.swiss/de/ist-daten-archiv/) provides a structured dataset of the public transport in Switzerland.

-   Every month is provided as \*.zip-File
-   Every day is provided as \*.csv-File
-   Every row contains the arrival or departure of a public vehicle

### Reduce dataset

The whole dataset for the year 2024 is roughly \~150GB. Working with such a big data set in R is very time consuming and impractical. Therefore the dataset was reduce to a manageable size before working with R.

The filtering process below reduces the size of the final csv-File to only \~26MB which is way easier to handle than \~150GB.

| Step |                                                            Command                                                             |                       Description                        |
|:----------------------:|:----------------------:|:----------------------:|
|  1   |    [bash download_zip_files.sh](https://github.com/paACode/preprocessing_opentransportdata/blob/main/download_zip_files.sh)    |      Downloads all \*.zip files from the year 2024       |
|  2   | [bash extract_luzern_to_csv.sh](https://github.com/paACode/preprocessing_opentransportdata/blob/main/extract_luzern_to_csv.sh) | Filters for "trains" of "SBB" with destination "Lucerne" |

In the end the colnames needed to be added again as they got lost during filtering.

<details>

<summary>Click to see Shell command</summary>

```         
sed -i "1i BETRIEBSTAG;FAHRT_BEZEICHNER;BETREIBER_ID;BETREIBER_ABK;BETREIBER_NAME;PRODUKT_ID;LINIEN_ID;LINIEN_TEXT;UMLAUF_ID;VERKEHRSMITTEL_TEXT;ZUSATZFAHRT_TF;FAELLT_AUS_TF;BPUIC;HALTESTELLEN_NAME;ANKUNFTSZEIT;AN_PROGNOSE;AN_PROGNOSE_STATUS;ABFAHRTSZEIT;AB_PROGNOSE;AB_PROGNOSE_STATUS;DURCHFAHRT_TF" luzern_arrrivals_only.csv
```

</details>

<p>

### Data Preparation

```{r Import and Validate}
#import libraries
library(testthat)
library(dplyr)
library(ggplot2)
#general settings
Sys.setenv(LANG = "en")
#
d.train.raw <-  read.csv(file = "raw_transport_2024.csv", sep = ";")

test_that("Shell Scripts to reduce dataset worked?", {
  expect_equal(unique(d.train.raw$BETREIBER_ABK), "SBB")
  expect_equal(unique(d.train.raw$HALTESTELLEN_NAME), "Luzern")
  expect_equal(unique(d.train.raw$PRODUKT_ID), "Zug")
})
```

```{r Clean Up}
# Columns needed: 
# More Information: https://opentransportdata.swiss/de/cookbook/actual-data/
# "BETRIEBSTAG" : categorizing weekday/month/season, merging weather 
# "LINIEN_TEXT" :describing the train category IC, S1 , IR16
# "ANKUNFTSZEIT" : calculation of delay
# "AN_PROGNOSE" : calculation of delay

col.needed <-  c("BETRIEBSTAG",
                 "LINIEN_TEXT",
                 "ANKUNFTSZEIT",
                 "AN_PROGNOSE")

#Filter 
d.train.filtered <-  d.train.raw %>%
  filter(ANKUNFTSZEIT != "") %>% #Remove trains starting from lucerne
  filter(AN_PROGNOSE != "") %>% #Remove trains where delay calc. not possible
  filter(AN_PROGNOSE_STATUS == "REAL") %>%  #Only consider REAL arrival times
  select(all_of(col.needed)) 

```

```{r Complete Dataset}

calc.delay.s <- function(exp.time,
                         exp.time.format,
                         actual.time,
                         actual.time.format) {
  test_that("Correct Function inputs?", {
    expect_equal(length(exp.time), length(actual.time))
    expect_equal(
      class(exp.time.format),
      class(actual.time.format),
      class(exp.time),
      class(actual.time),
      "character"
    )
  })
  
  actual.time_ <- as.POSIXct(actual.time, format = actual.time.format)
  exp.time_ <- as.POSIXct(exp.time, format = exp.time.format)
  delay.s <- as.numeric(difftime(actual.time_, exp.time_))
  return(delay.s)
  
}

#Create dataset with complimentary information
d.train.compl <-  d.train.filtered

#Add delay in seconds
d.train.compl$delay.calc.s <- calc.delay.s(
  exp.time = d.train.filtered$ANKUNFTSZEIT,
  exp.time.format = "%d.%m.%Y %H:%M",# SBB provides min accuracy
  actual.time = d.train.filtered$AN_PROGNOSE,
  actual.time.format = "%d.%m.%Y %H:%M:%S" # Meas. accuracy is in seconds
)
#Add delay in minutes
d.train.compl$delay.calc.min <- round(d.train.compl$delay.calc.s/60, 3)

#Add delay category
d.train.compl$delay.category <- cut(
  x = d.train.compl$delay.calc.s,
  breaks = c(-Inf, 0, 5, 15, 30, Inf) * 60,# because it is in seconds
  labels = c("on time", "<5 min", "5-15 min", "15-30 min", ">30 min")
)

#Add Weekdays
d.train.compl$weekday <- weekdays(as.Date(d.train.compl$BETRIEBSTAG, 
                                          format = "%d.%m.%Y"))
#Add months
d.train.compl$month <- months(as.Date(d.train.compl$BETRIEBSTAG, 
                                          format = "%d.%m.%Y"))
#Add season
d.train.compl$season <- case_when(
  d.train.compl$month %in% c("December", "January", "February") ~ "Winter",
  d.train.compl$month %in% c("March", "April", "May") ~ "Spring",
  d.train.compl$month %in% c("June", "July", "August") ~ "Summer",
  d.train.compl$month %in% c("September", "October", "November") ~ "Fall"
)

```

```{r First Plot}

# Create the histogram with percentages
ggplot(data =d.train.compl, aes(x = delay.category)) +
  geom_bar(aes(y = ..count.. / sum(..count..) * 100),  # Convert counts to percentages
                 fill = "skyblue", color = "black") +  # Customize the appearance
  labs(y = "Count[%]", x = "Delay [mins]") +  # Add axis labels
  #scale_x_sqrt()+
  theme_minimal()  # Apply a minimal theme




```

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

## Weather Data

```{r echo=TRUE}
## Import weather data from CSV file.
d.weather <- read.csv(file = "raw_weather_2024.csv", sep = ",")
```

The Federal Office of Meteorology and Climatology MeteoSwiss publishes a wide range of data, including the dataset <a href="https://www.meteoschweiz.admin.ch/wetter/messsysteme/bodenstationen/schweizer-klimamessnetz.html">Schweizer Klimamessnetz</a>. This dataset contains the most climatologically significant ground-based measurement stations within MeteoSwiss's measurement network. It consists of 29 climate stations and 46 precipitation stations and includes daily average values of, for example, total snow depth, sunshine duration, precipitation, and air temperature.

### Data Preprocessing

The measurement data from <a href="https://www.meteoschweiz.admin.ch/wetter/messsysteme/bodenstationen/schweizer-klimamessnetz.html">Schweizer Klimamessnetz</a> is available in separate CSV files for each station and day. To maintain manageable complexity for analysis within the scope of this module, all measurement values from the 29 different measurement stations were aggregated, and an average value per measurement unit per day was calculated. This preprocessing was performed using a Python script.

<details>

<summary>Click to see Python code</summary>

``` python
import pandas as pd
import glob
import numpy as np

# Step 1: Define file paths
input_folder = "/Users/leonarddost/Documents/RBootcamp/raw_data"
output_file = "/Users/leonarddost/Documents/RBootcamp/swiss_avg_weather_2024.csv"

# Step 2: Load all CSV files with the correct delimiter
file_paths = glob.glob(f"{input_folder}/*.csv")
dfs = [pd.read_csv(file, delimiter=';') for file in file_paths]

# Step 3: Concatenate the data
combined_df = pd.concat(dfs, ignore_index=True)

# Debugging: Check the columns in the combined DataFrame
print("Column names in the combined DataFrame:", combined_df.columns)

# Step 4: Replace missing values ("-") with NaN
combined_df.replace("-", np.nan, inplace=True)

# Step 5: Convert numeric columns to numeric types (coerce invalid values to NaN)
columns_to_aggregate = [col for col in combined_df.columns if col not in ['station/location', 'date']]

for col in columns_to_aggregate:
    combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')

# Debugging: Print rows with NaN values to check for invalid data
invalid_rows = combined_df[combined_df[columns_to_aggregate].isnull().any(axis=1)]
if not invalid_rows.empty:
    print("Rows with invalid numeric data detected:")
    print(invalid_rows)

# Step 6: Group by 'date' and calculate averages for numeric columns
avg_df = combined_df.groupby('date')[columns_to_aggregate].mean().reset_index()

# Step 7: Rename the columns
column_mapping = {
    'station/location': 'Wetterstation',
    'date': 'Datum',
    'gre000d0': 'Globalstrahlung',
    'hto000d0': 'Gesamtschneehöhe',
    'nto000d0': 'Gesamtbewölkung',
    'prestad0': 'Luftdruck',
    'rre150d0': 'Niederschlag',
    'sre000d0': 'Sonnenscheindauer',
    'tre200d0': 'Lufttemperatur',
    'tre200dn': 'Lufttemperatur (min)',
    'tre200dx': 'Lufttemperatur (max)',
    'ure200d0': 'Luftfeuchtigkeit'
}

avg_df.rename(columns=column_mapping, inplace=True)

# Step 8: Export the final DataFrame to CSV
avg_df.to_csv(output_file, index=False)

print(f"The merged file with updated column names has been saved to: {output_file}")
```

</details>

<p>

In addition to merging and averaging the individual measurement values, the dataset's labels were also renamed for readability. For example, *tre200d0* was renamed to *Lufttemperatur*.

```{r echo=FALSE, include=TRUE, message=FALSE}
## Display weather data
library(dplyr)

d.weather %>%
  transmute(
    Datum = Datum,
    Sonnenscheindauer = round(Sonnenscheindauer, 2),
    Niederschlag = round(Niederschlag, 1),
    Temperatur = round(Lufttemperatur, 1)
  ) %>%
  head()

```

Finally, we obtained an average weather dataset for Switzerland for the year 2024, based on the most important ground measurement stations within the MeteoSwiss network.

## Merge train data with weather data

### Prepare dataset for merge

In order to carry out the merge it was observed that the data format of the data set *raw_weather_2024.csv* was different to the format of the *raw_transport_2024.csv*. Therefor the format was manipulated so that both datasets have the same date formate.

```{r echo=TRUE, include=TRUE}
## Transform date format for further analysis
## Ensure the column is interpreted as a character string
d.weather$Datum <- as.character(d.weather$Datum)

## Format the date correctly
d.weather$Datum <- format(as.Date(d.weather$Datum, format = "%Y%m%d"), "%d.%m.%Y")

## Check the result
head(d.weather$Datum)

```

### Merge the data

Afterwards, to merge the data a left join method was chosen and carried out. The ID used for the merge was the prior prepared date column.

```{r echo=TRUE, include=TRUE}
## Merge Weather Data into Train Dataset

d.combined <- d.train.compl %>% 
  left_join(d.weather, by = c("BETRIEBSTAG" = "Datum"))

```

# Data Analysis

This analysis chapter examines the temporal and categorical patterns of train delays affecting MadeUpCompany AG employees arriving in Lucerne. By analyzing public transport data from 2024 onward, combined with Swiss weather data, the study investigates delay frequency, duration, and their distribution across weekdays and months. Key metrics, including total and delayed train counts, delay severity, and weather influence, are evaluated to identify trends that could inform effective countermeasures. Through statistical summaries and visualizations, the analysis aims to provide precise, data-driven recommendations to enhance production reliability by mitigating transport-related disruptions.

## Overview of Data

As a first step to get an better understanding of the data set we plotted delays and air temperature. From this first impression plot, we observed that there does not seem to be a clear, strong linear relationship between air temperature ("Lufttemperatur") and the delay. The points are widely scattered and the regression line appears mostly flat. Further, there is a dense clustering of points around zero on the y-axis, indicating that a large number of observations have minimal or no delays, regardless of the air temperature. There are several noticeable outliers where delays exceed 50 minutes, but these events seem sporadic and do not follow a clear pattern with air temperature. The air temperature values primarily fall between 0 and 20 degrees, but within this range, no significant change in delays is visually obvious. However, between 0 and -5 degrees there seems to be a higher delay suggesting further analysis also regarding percipitation. Although the use of jittering is helping to distinguish overlapping points, the overall impression is that delays are not highly influenced by air temperature.

```{r Delay in relation to air temperature, include=TRUE, message=FALSE}
ggplot(d.combined, aes(x = Lufttemperatur, y = delay.calc.min)) +
  geom_jitter(color = "blue", width = 0.2, height = 0.2, alpha = 0.6) + 
  geom_smooth(method = "lm", color = "orange", se = FALSE) + 
  labs(title = "Scatter Plot with Jitter and Regression Line",
       x = "Lufttemperatur",
       y = "Delay (min)") +
  theme_minimal()
```

The "Scatter Plot with Jitter and Regression Line" serves as a representative example of the weak relationship observed between weather variables and train delays overall. This suggests that while weather may not be a primary driver of delays, further analysis of potential correlations between delayed trains and specific weather conditions could still reveal hidden patterns. However, before delving into these correlations, it is essential to first conduct a detailed analysis of delay patterns at the weekday and seasonal levels, as seen in the current summary table.

The summary table of train delays by weekday reveals significant differences in train performance across the week. Weekdays generally experience more frequent and longer delays compared to weekends, with **Tuesday** recording the highest number of delayed trains at **8'458** and a high delay percentage of **68.05%**. Similarly, **Thursday** exhibits a high delay percentage of **67.29%** and the longest average delay time of **113.31** seconds, indicating that these days face substantial operational challenges. In contrast, **Saturday** shows the lowest number of delayed trains (**6'572**) and the shortest average delay time (**79.83 seconds**), making it the most efficient day. The weekend, overall, has a lower percentage of delayed trains, with **Saturday (54.36%)** and **Sunday (57.61%)** performing significantly better than most weekdays. This pattern suggests that weekdays, particularly **Tuesday** and **Thursday**, experience higher train traffic and potential congestion, requiring further investigation into scheduling or operational adjustments to reduce delays.

```{r Overview Weekdays Table, echo=FALSE, message = FALSE, include=TRUE}
library(dplyr)
library(gt)

## Step 1: Prepare the table (same as before)
## create subset with only delayed trains
d.delayed_trains <- d.combined %>% 
  filter(delay.category != "on time")

## count total trains per weekday
total_trains_per_day <- d.combined %>%
  group_by(weekday) %>%
  summarise(total_trains = n())

## create default table with train count, delay avg 
## and delayed train count by weekday
overview_delay_table <- d.delayed_trains %>%
  group_by(weekday) %>%
  summarise(delay_count = n(),
            avg_delay_seconds = mean(delay.calc.s, na.rm = TRUE)) %>%
  left_join(total_trains_per_day, by = "weekday") %>%
  mutate(
    percentage_delayed_trains = (delay_count / total_trains) * 100,
    weekday = factor(
      weekday,
      levels = c(
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday"
      )
    )
  ) %>%
  arrange(weekday)

# Step 2: Create a beautiful table using `gt`
overview_delay_table %>%
  gt() %>%
  tab_header(title = "Summary of Train Delays by Weekday", subtitle = "Total trains, delayed trains, and percentage delays") %>%
  fmt_number(
    columns = c(avg_delay_seconds, percentage_delayed_trains),
    decimals = 2
  ) %>%
  cols_label(
    weekday = "weekday",
    delay_count = "Delayed Trains",
    avg_delay_seconds = "Average Delay (s)",
    total_trains = "Total Trains",
    percentage_delayed_trains = "Delayed (%)"
  )

```
Similarly to the prior table, the following graph reveals delay patterns on a monthly level.

```{r Overview Months Table, echo=FALSE, message = FALSE, include=FALSE}
library(dplyr)
library(gt)

## Step 1: Prepare the table for months (similar to the weekday table)
## count total trains per month
total_trains_per_month <- d.combined %>%
  group_by(month) %>%
  summarise(total_trains = n())

## create table with train count, delay avg, and delayed train count by month
overview_month_delay_table <- d.delayed_trains %>%
  group_by(month) %>%
  summarise(delay_count = n(),
            avg_delay_seconds = mean(delay.calc.s, na.rm = TRUE)) %>%
  left_join(total_trains_per_month, by = "month") %>%
  mutate(
    percentage_delayed_trains = (delay_count / total_trains) * 100,
    month = factor(
      month,
      levels = c(
        "January", "February", "March", "April", "May", "June", 
        "July", "August", "September", "October", "November", "December"
      )
    )
  ) %>%
  arrange(month)

# Step 2: Create a beautiful table using `gt`
overview_month_delay_table %>%
  gt() %>%
  tab_header(title = "Summary of Train Delays by Month", subtitle = "Total trains, delayed trains, and percentage delays") %>%
  fmt_number(
    columns = c(avg_delay_seconds, percentage_delayed_trains),
    decimals = 2
  ) %>%
  cols_label(
    month = "Month",
    delay_count = "Delayed Trains",
    avg_delay_seconds = "Average Delay (s)",
    total_trains = "Total Trains",
    percentage_delayed_trains = "Delayed (%)"
  )

```
```{r Overview Months Delays, echo=FALSE, message = FALSE, include=TRUE}
# Reshape the table for long format (needed for facetting)
library(tidyr)

long_delay_table <- overview_month_delay_table %>%
  pivot_longer(cols = c(delay_count, avg_delay_seconds, percentage_delayed_trains),
               names_to = "Metric", values_to = "Value")

# Create the faceted plot
ggplot(long_delay_table, aes(x = month, y = Value, fill = Metric)) +
  geom_col(position = "dodge") +
  facet_wrap(~ Metric, scales = "free_y", ncol = 1) +
  scale_fill_manual(values = c("steelblue", "darkorange", "darkgreen")) +
  labs(
    title = "Monthly Overview of Train Delays",
    x = "Month",
    y = "Value"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r Average Delay Duration by Month, , echo=FALSE, message = FALSE, include=TRUE}
ggplot(overview_month_delay_table, aes(x = month, y = avg_delay_seconds, group = 1)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(size = 2, color = "darkred") +
  labs(
    title = "Average Delay Duration by Month",
    x = "Month",
    y = "Average Delay (seconds)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Question: At which weekday are more extreme delays? -> Boxplot with the outliers could show 

Question: At which time e.g. on thursdays is the delay the most?

# Results

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

# Conclusion

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.
