---
title: "Paleo an opentransport Analysis"
author: "Leo Dost and Pascal Ackermann"
date: "30.01.2025"
output: html_document
---

```{r, echo=FALSE , include=FALSE}
## we remove the printing of the hashes in front of all results
library(knitr)
opts_chunk$set(echo = FALSE ,
               include = FALSE,
               comment = NA)
```

# Introduction

MadeUpCompany AG has recently identified that production employees frequently arrive late to work due to delays in public transportation. This poses a significant challenge since the absence of a single team member can halt the entire production line.

To address this issue, the Production Manager has initiated a research project in collaboration with HSLU.

Three possible countermeasure were identified:

-   Adjust shift start times based on transportation reliability.
-   Implement seasonal adjustments to production capacity based on weather-related transportation disruptions.
-   Consider train categories to mitigate dependency on unreliable train routes.
-   Use weather data and forecasts to predict possible delays

The goal of the Data Science students is to provide MadeUpCompany AG with an insightful report, analyzing publicly available data, to see which of the countermeasures could be most effective.

# Exploring Data

The first step in addressing the research question was to define the project scope and select suitable datasets. The analysis focuses specifically on public transport data for trains arriving in Lucerne, as MadeUpCompany AG is based there. Data from 2024 onward serves as the foundation for more advanced analysis. Additionally, weather data should reflect average Swiss values as a daily baseline.

For this study, data was sourced from [opentransportdata.swiss](https://opentransportdata.swiss/de/ist-daten-archiv/) and the [Schweizer Klimamessnetz](https://www.meteoschweiz.admin.ch/wetter/messsysteme/bodenstationen/schweizer-klimamessnetz.html).

## Tranport Data
[opentransportdata.swiss](https://opentransportdata.swiss/de/ist-daten-archiv/) provides a structured dataset of the public transport in Switzerland.

- Every month is provided as *.zip-File
- Every day is provided as *.csv-File
- Every row contains the arrival or departure of a public vehicle

### Reduce dataset

The whole dataset for the year 2024 is roughly ~150GB. Working with such a big data set in R is very time consuming and impractical. Therefore the dataset was reduce to a manageable size before working with R.

The filtering process below reduces the size of the final csv-File to only ~26MB which is way easier to handle than ~150GB.


Step | Command | Description
:-: | :-: | :-:
1| [bash download_zip_files.sh](https://github.com/paACode/preprocessing_opentransportdata/blob/main/download_zip_files.sh)| Downloads all *.zip files from the year 2024
2| [bash extract_luzern_to_csv.sh](https://github.com/paACode/preprocessing_opentransportdata/blob/main/extract_luzern_to_csv.sh)|  Filters for "trains" of "SBB" with destination "Lucerne"

In the end the colnames needed to be added again as they got lost during filtering.

<details>

<summary>Click to see Shell command </summary>

``` 
sed -i "1i BETRIEBSTAG;FAHRT_BEZEICHNER;BETREIBER_ID;BETREIBER_ABK;BETREIBER_NAME;PRODUKT_ID;LINIEN_ID;LINIEN_TEXT;UMLAUF_ID;VERKEHRSMITTEL_TEXT;ZUSATZFAHRT_TF;FAELLT_AUS_TF;BPUIC;HALTESTELLEN_NAME;ANKUNFTSZEIT;AN_PROGNOSE;AN_PROGNOSE_STATUS;ABFAHRTSZEIT;AB_PROGNOSE;AB_PROGNOSE_STATUS;DURCHFAHRT_TF" luzern_arrrivals_only.csv
```
</details>

<p>


### Data Preparation



```{r Import and Validate}
#import libraries
library(testthat)
library(dplyr)
library(ggplot2)
#general settings
Sys.setenv(LANG = "en")
#
d.train.raw <-  read.csv(file = "raw_transport_2024.csv", sep = ";")

test_that("Shell Scripts to reduce dataset worked?", {
  expect_equal(unique(d.train.raw$BETREIBER_ABK), "SBB")
  expect_equal(unique(d.train.raw$HALTESTELLEN_NAME), "Luzern")
  expect_equal(unique(d.train.raw$PRODUKT_ID), "Zug")
})
```



```{r Clean Up}
# Columns needed: 
# More Information: https://opentransportdata.swiss/de/cookbook/actual-data/
# "BETRIEBSTAG" : categorizing weekday/month/season, merging weather 
# "LINIEN_TEXT" :describing the train category IC, S1 , IR16
# "ANKUNFTSZEIT" : calculation of delay
# "AN_PROGNOSE" : calculation of delay

col.needed <-  c("BETRIEBSTAG",
                 "LINIEN_TEXT",
                 "ANKUNFTSZEIT",
                 "AN_PROGNOSE")

#Filter 
d.train.filtered <-  d.train.raw %>%
  filter(ANKUNFTSZEIT != "") %>% #Remove trains starting from lucerne
  filter(AN_PROGNOSE != "") %>% #Remove trains where delay calc. not possible
  filter(AN_PROGNOSE_STATUS == "REAL") %>%  #Only consider REAL arrival times
  select(all_of(col.needed)) 

```



```{r Complete Dataset}

#' Calculate delay
#'
#' This function calculates the  delay of arriving trains.
#'
#' @param exp.time 
#' @param exp.time.format 
#' @param actual.time  
#'
#' @return delay in seconds
#' @examples
#' # Calculate the area of a 5 by 10 rectangle
#' calculate_area(5, 10)
calc.delay.s <- function(exp.time,
                         exp.time.format,
                         actual.time,
                         actual.time.format) {
  test_that("Correct Function inputs?", {
    expect_equal(length(exp.time), length(actual.time))
    expect_equal(
      class(exp.time.format),
      class(actual.time.format),
      class(exp.time),
      class(actual.time),
      "character"
    )
  })
  
  actual.time_ <- as.POSIXct(actual.time, format = actual.time.format)
  exp.time_ <- as.POSIXct(exp.time, format = exp.time.format)
  
  delay.s <- as.numeric(difftime(exp.time_, actual.time_))
  return(delay.s)
  
}
#Create a new dataset that will be complemented with additional information
d.train.compl <-  d.train.filtered

#Calculate Delay and add to Dataset
d.train.compl$delay.calc.s <- calc.delay.s(
  exp.time = d.train.filtered$ANKUNFTSZEIT,
  exp.time.format = "%d.%m.%Y %H:%M",# SBB provides min accuracy
  actual.time = d.train.filtered$AN_PROGNOSE,
  actual.time.format = "%d.%m.%Y %H:%M:%S" # Meas accuracy in seconds
)



#Add Weekdays
d.train.compl$weekday <- weekdays(as.Date(d.train.compl$BETRIEBSTAG, 
                                          format = "%d.%m.%Y"))

#...

```

```{r First Plot}

#First Histogramm Plot

```



Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

## Weather Data

```{r echo=TRUE}
## Import weather data from CSV file.
d.weather <- read.csv(file = "raw_weather_2024.csv", sep = ",")
```

The Federal Office of Meteorology and Climatology MeteoSwiss publishes a wide range of data, including the dataset *<a href="https://www.meteoschweiz.admin.ch/wetter/messsysteme/bodenstationen/schweizer-klimamessnetz.html">Schweizer Klimamessnetz</a>*. This dataset contains the most climatologically significant ground-based measurement stations within MeteoSwiss's measurement network. It consists of 29 climate stations and 46 precipitation stations and includes daily average values of, for example, total snow depth, sunshine duration, precipitation, and air temperature.

### Data Preprocessing

The measurement data from *<a href="https://www.meteoschweiz.admin.ch/wetter/messsysteme/bodenstationen/schweizer-klimamessnetz.html">Schweizer Klimamessnetz</a>* is available in separate CSV files for each station and day. To maintain manageable complexity for analysis within the scope of this module, all measurement values from the 29 different measurement stations were aggregated, and an average value per measurement unit per day was calculated. This preprocessing was performed using a Python script.

<details>

<summary>Click to see Python code</summary>

``` python
import pandas as pd
import glob
import numpy as np

# Step 1: Define file paths
input_folder = "/Users/leonarddost/Documents/RBootcamp/raw_data"
output_file = "/Users/leonarddost/Documents/RBootcamp/swiss_avg_weather_2024.csv"

# Step 2: Load all CSV files with the correct delimiter
file_paths = glob.glob(f"{input_folder}/*.csv")
dfs = [pd.read_csv(file, delimiter=';') for file in file_paths]

# Step 3: Concatenate the data
combined_df = pd.concat(dfs, ignore_index=True)

# Debugging: Check the columns in the combined DataFrame
print("Column names in the combined DataFrame:", combined_df.columns)

# Step 4: Replace missing values ("-") with NaN
combined_df.replace("-", np.nan, inplace=True)

# Step 5: Convert numeric columns to numeric types (coerce invalid values to NaN)
columns_to_aggregate = [col for col in combined_df.columns if col not in ['station/location', 'date']]

for col in columns_to_aggregate:
    combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')

# Debugging: Print rows with NaN values to check for invalid data
invalid_rows = combined_df[combined_df[columns_to_aggregate].isnull().any(axis=1)]
if not invalid_rows.empty:
    print("Rows with invalid numeric data detected:")
    print(invalid_rows)

# Step 6: Group by 'date' and calculate averages for numeric columns
avg_df = combined_df.groupby('date')[columns_to_aggregate].mean().reset_index()

# Step 7: Rename the columns
column_mapping = {
    'station/location': 'Wetterstation',
    'date': 'Datum',
    'gre000d0': 'Globalstrahlung',
    'hto000d0': 'Gesamtschneehöhe',
    'nto000d0': 'Gesamtbewölkung',
    'prestad0': 'Luftdruck',
    'rre150d0': 'Niederschlag',
    'sre000d0': 'Sonnenscheindauer',
    'tre200d0': 'Lufttemperatur',
    'tre200dn': 'Lufttemperatur (min)',
    'tre200dx': 'Lufttemperatur (max)',
    'ure200d0': 'Luftfeuchtigkeit'
}

avg_df.rename(columns=column_mapping, inplace=True)

# Step 8: Export the final DataFrame to CSV
avg_df.to_csv(output_file, index=False)

print(f"The merged file with updated column names has been saved to: {output_file}")
```

</details>

<p>

In addition to merging and averaging the individual measurement values, the dataset's labels were also renamed for readability. For example, *tre200d0* was renamed to *Lufttemperatur*.

```{r eval=TRUE, echo=FALSE, include=TRUE, message=FALSE}
## Display weather data
library(dplyr)

d.weather %>%
  transmute(
    Datum = Datum,
    Sonnenscheindauer = round(Sonnenscheindauer, 2),
    Niederschlag = round(Niederschlag, 1),
    Temperatur = round(Lufttemperatur, 1)
  ) %>%
  head()

```


Finally, we obtained an average weather dataset for Switzerland for the year 2024, based on the most important ground measurement stations within the MeteoSwiss network.

## Merge train data with weather data

### Prepare dataset for merge
In order to carry out the merge it was observed that the data format of the data set *raw_weather_2024.csv* was different to the format of the *raw_transport_2024.csv*. Therefor the format was manipulated so that both datasets have the same date formate. 

```{r eval=FALSE, echo=TRUE, include=TRUE}
## Transform date format for further analysis
## Ensure the column is interpreted as a character string
d.weather$Datum <- as.character(d.weather$Datum)

## Format the date correctly
d.weather$Datum <- format(as.Date(d.weather$Datum, format = "%Y%m%d"), "%d.%m.%Y")

## Check the result
head(d.weather$Datum)

```

Afterwards, to merge the data a left join method was chosen and carried out. The ID used for the merge was the prior prepared date column.

```{r eval=FALSE, echo=TRUE, include=TRUE}
## Merge Weather Data into Train Dataset

d.combined <- train_data_filtered %>% 
  left_join(d.weather, by = c("BETRIEBSTAG" = "Datum"))

```


# Data Analysis

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

# Results

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.

# Conclusion

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.
